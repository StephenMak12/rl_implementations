{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of this code is a barebones implementation of Vanilla Policy Gradient (a.k.a REINFORCE / Monte Carlo Policy Gradient) for the CartPole-v0 environment. This solves the problem is approx. 200 episodes.\n",
    "\n",
    "This code has not been fully unit tested / using TensorBoard / saving videos etc. to remove as much complexity as possible.\n",
    "\n",
    "Code with the above bells and whistles will be held in a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T22:30:22.546847Z",
     "start_time": "2020-10-24T22:30:21.280674Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import gym\n",
    "\n",
    "sys.path.append('../../../01_vrp/src/vrp/utils/')\n",
    "import sm_functions as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T22:30:22.558563Z",
     "start_time": "2020-10-24T22:30:22.547849Z"
    }
   },
   "outputs": [],
   "source": [
    "class VanillaPolicyGradient:\n",
    "    def __init__(self, gamma, learning_rate):\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        self.model = self._build_model()\n",
    "        self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        \n",
    "    def compute_action(self, state):\n",
    "        '''Calculates $\\pi(s, a)$'''\n",
    "        probs = self.model(tf.convert_to_tensor([state]))\n",
    "        action_probs = tfp.distributions.Categorical(probs=probs)\n",
    "        action = action_probs.sample()\n",
    "        return action.numpy()[0]\n",
    "    \n",
    "    def learn(self, episode):\n",
    "        '''After an episode, the agent loops through its memory and learns.'''\n",
    "        self.discounted_return_memory = self._discount(self.reward_memory)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            total_loss = 0\n",
    "\n",
    "            for idx, (state, discounted_return) in enumerate(zip(self.state_memory, self.discounted_return_memory)):\n",
    "                \n",
    "                state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "                probs = self.model(state)\n",
    "                action_probs = tfp.distributions.Categorical(probs=probs)\n",
    "                log_probs = action_probs.log_prob(self.action_memory[idx])\n",
    "                \n",
    "                total_loss += -1 * discounted_return * tf.squeeze(log_probs)\n",
    "                        \n",
    "            grads = tape.gradient(total_loss, self.model.trainable_weights)\n",
    "            self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "            \n",
    "            with tensorboard_writer.as_default():\n",
    "                tf.summary.scalar('Loss', total_loss, step=episode)\n",
    "                \n",
    "                for weights, gradient in zip(self.model.trainable_weights, grads):\n",
    "                    tf.summary.histogram(weights.name, weights, step=episode)\n",
    "                    tf.summary.histogram(weights.name + '_grads', gradient, step=episode)\n",
    "                    \n",
    "                tensorboard_writer.flush()\n",
    "        \n",
    "        # REMEMBER TO RESET MEMORY!!\n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "\n",
    "        return self.discounted_return_memory\n",
    "    \n",
    "    def store_transitions(self, state, action, reward):\n",
    "        '''Store individual state, action, reward transitions.'''\n",
    "        self.state_memory.append(state)\n",
    "        self.action_memory.append(action)\n",
    "        self.reward_memory.append(reward)\n",
    "            \n",
    "    def _discount(self, reward_memory):\n",
    "        '''Calculate v(s) for all states encountered'''\n",
    "        cum_reward = 0\n",
    "        discounted_return_memory = []\n",
    "        reward_memory.reverse()\n",
    "        \n",
    "        for reward in reward_memory:\n",
    "            cum_reward = reward + self.gamma * cum_reward\n",
    "            discounted_return_memory.append(cum_reward)\n",
    "        discounted_return_memory.reverse()\n",
    "        \n",
    "        return discounted_return_memory\n",
    "            \n",
    "    def _build_model(self):\n",
    "        '''Define neural network'''\n",
    "        inputs = tf.keras.Input(shape=(4,))\n",
    "        x = tf.keras.layers.Dense(256, activation='relu')(inputs)\n",
    "        x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "        outputs = tf.keras.layers.Dense(2, activation='softmax')(x)\n",
    "        \n",
    "        model = tf.keras.Model(inputs=inputs, outputs=outputs, name='VPG')\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T22:40:55.895966Z",
     "start_time": "2020-10-24T22:32:42.664780Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Currently not saving any configs, logs or scripts\n",
      "Model: \"VPG\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               1280      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 67,586\n",
      "Trainable params: 67,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "This took 493.2263717651367 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "FLAG_SAVE = False\n",
    "FOLDER_DESCRIPTION = 'cartpole'\n",
    "FLAG_PROD = False\n",
    "PATH_SAVE_RUNS = '../../runs/'\n",
    "\n",
    "# Save all configs, scripts and logs (if desired) so that experiments are repeatable.\n",
    "FOLDER_PATH_FULL = sm.generate_runs_folder(flag_save=FLAG_SAVE,\n",
    "                                           folder_description=FOLDER_DESCRIPTION,\n",
    "                                           flag_prod=FLAG_PROD,\n",
    "                                           path_save_runs=PATH_SAVE_RUNS,\n",
    "                                           path_config=None,\n",
    "                                           path_train=None,\n",
    "                                           path_env=None,\n",
    "                                           path_policy=None)\n",
    "\n",
    "start = time.time()\n",
    "gamma = 0.99\n",
    "learning_rate = 0.0005\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "agent = VanillaPolicyGradient(gamma=gamma, learning_rate=learning_rate)\n",
    "print(agent.model.summary())\n",
    "\n",
    "tensorboard_writer = tf.summary.create_file_writer(FOLDER_PATH_FULL)\n",
    "\n",
    "n_episodes = 1000\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    returns = 0\n",
    "\n",
    "    # Play an episode\n",
    "    while not done:\n",
    "        action = agent.compute_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        agent.store_transitions(state, action, reward)\n",
    "        state = next_state\n",
    "        returns += reward\n",
    "\n",
    "    # Learn from said episode\n",
    "    agent.learn(episode)\n",
    "\n",
    "    with tensorboard_writer.as_default():\n",
    "        tf.summary.scalar('Return', returns, step=episode)\n",
    "        \n",
    "    env.close()\n",
    "    \n",
    "print(f'This took {time.time() - start} seconds to run.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('rl': conda)",
   "language": "python",
   "name": "python361064bitrlconda66c8bc5abfd041dcb87b4e65206e9e66"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
